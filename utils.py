import os
import glob
import fitz  # PyMuPDF
import spacy
import re
import copy
from wtpsplit import SaT
import torch

def extract_text_from_pdfs(input_folder, output_raw_txt="merged_raw.txt"):
    """í´ë” ë‚´ì˜ ëª¨ë“  PDFë¥¼ ì½ì–´ í…ìŠ¤íŠ¸ íŒŒì¼ í•˜ë‚˜ë¡œ ë³‘í•©"""
    print(f"ğŸ“‚ PDF ì¶”ì¶œ ì‹œì‘: {input_folder}")
    
    # PDF íŒŒì¼ ë¦¬ìŠ¤íŠ¸ í™•ë³´
    pdf_files = glob.glob(os.path.join(input_folder, '**', '*.pdf'), recursive=True)
    if not pdf_files:
        print("âŒ ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # spaCy ë¡œë“œ (ë¬¸ì¥ ë‹¨ìœ„ 1ì°¨ ë¶„ë¦¬ìš©)
    try:
        nlp = spacy.load("en_core_web_sm")
    except OSError:
        from spacy.cli import download
        download("en_core_web_sm")
        nlp = spacy.load("en_core_web_sm")
    
    # íŒŒì´í”„ë¼ì¸ ìµœì í™”
    disable_list = ["ner", "tagger", "attribute_ruler", "lemmatizer"]
    if "senter" in nlp.pipe_names: disable_list.append("parser")
    nlp.disable_pipes(disable_list)
    if "senter" not in nlp.pipe_names: nlp.add_pipe("senter")

    all_sentences = []

    for pdf_path in pdf_files:
        try:
            doc = fitz.open(pdf_path)
            for page in doc:
                text = page.get_text()
                if not text: continue
                # spaCyë¡œ 1ì°¨ ë¬¸ì¥ ë¶„ë¦¬ ë° ì¤„ë°”ê¿ˆ ì²˜ë¦¬
                spacy_doc = nlp(text)
                for sent in spacy_doc.sents:
                    clean_sent = sent.text.strip()
                    if clean_sent:
                        all_sentences.append(clean_sent)
            doc.close()
            print(f"  - ì²˜ë¦¬ ì™„ë£Œ: {os.path.basename(pdf_path)}")
        except Exception as e:
            print(f"  - ì˜¤ë¥˜ ë°œìƒ ({pdf_path}): {e}")

    # ë³‘í•©ëœ í…ìŠ¤íŠ¸ ì €ì¥
    with open(output_raw_txt, "w", encoding="utf-8") as f:
        f.write("\n".join(all_sentences))
    
    print(f"âœ… 1ì°¨ ë³‘í•© ì™„ë£Œ: {output_raw_txt} (ì´ {len(all_sentences)} ë¬¸ì¥)")
    return output_raw_txt

def clean_text_logic(raw_text):
    """ì‚¬ìš©ìì˜ ì •ê·œì‹ ë¡œì§ì„ ì ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì •ì œ"""
    # 1. íŠ¹ìˆ˜ ë¬¸ì ë° ë¹„ì˜ì–´ê¶Œ ë¬¸ì ì œê±°ë¥¼ ìœ„í•œ íŒ¨í„´ ì‹ë³„
    # ì›ë³¸ ì½”ë“œ ë¡œì§: í•œê¸€/ì˜ì–´/ìˆ«ì ë“±ì„ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜ -> ë‚¨ì€ê±´ íŠ¹ìˆ˜ë¬¸ì/ì“°ë ˆê¸°ê°’
    garbage_check = re.sub(r"[ê°€-í£a-zA-Z0-9!?.\"]+", " ", raw_text)
    garbage_check = re.sub(r"[\s]", " ", garbage_check)
    garbage_list = list(set(garbage_check.split()))
    garbage_list.sort(key=len, reverse=True) # ê¸´ ê²ƒë¶€í„° ì œê±°

    # 2. í…ìŠ¤íŠ¸ ì •ì œ ì‹œì‘
    text = copy.deepcopy(raw_text)
    
    # ê¸°ë³¸ì ì¸ ì¹˜í™˜
    replacements = {
        "â€™": "'", "â€˜": "'", "â€œ": '"', "â€": '"',
        "Ã¡": "a", "ï¬‚": "fl", "Ã©": "e", "ï¬": "fi",
        "Ã¨": "e", "Ã‰": "E", "Ã¼": "u", "â€¦": "..."
    }
    for k, v in replacements.items():
        text = text.replace(k, v)

    # í•œê¸€ ì œê±°
    text = re.sub(r'[ê°€-í£]+', '', text)

    # 1ë²ˆì—ì„œ ì‹ë³„í•œ Garbage ë¬¸ì ì œê±°
    for trash in garbage_list:
        if trash:
            text = text.replace(trash, " ")

    # ìµœì¢… ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    return text

def refine_and_segment(input_txt_path, output_refined_txt="refined_text.txt"):
    """í…ìŠ¤íŠ¸ ì •ì œ í›„ SaT ëª¨ë¸ë¡œ ë¬¸ì¥ ë¶„ë¦¬"""
    print("ğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ ë° SaT ë¬¸ì¥ ë¶„ë¦¬ ì‹œì‘...")
    
    with open(input_txt_path, 'r', encoding='utf-8') as f:
        raw_text = f.read()

    # ì •ì œ í•¨ìˆ˜ í˜¸ì¶œ
    cleaned_text = clean_text_logic(raw_text)

    # SaT ëª¨ë¸ ë¡œë“œ
    sat = SaT("sat-12l-sm")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"  - SaT ëª¨ë¸ ë¡œë”© ì¤‘ (Device: {device})")
    if device == "cuda":
        sat.half().to(device)
    
    # ë¬¸ì¥ ë¶„ë¦¬
    seg_list = sat.split(cleaned_text)
    
    # ìµœì¢… ë¦¬ìŠ¤íŠ¸ ë‹¤ë“¬ê¸°
    final_sentences = []
    for seg in seg_list:
        seg = re.sub(r'\s+', ' ', seg).strip()
        if seg:
            final_sentences.append(seg)

    # íŒŒì¼ ì €ì¥ (í•œ ì¤„ì— í•œ ë¬¸ì¥)
    with open(output_refined_txt, "w", encoding="utf-8") as f:
        f.write("\n".join(final_sentences))

    print(f"âœ¨ ì •ì œ ë° ë¶„ë¦¬ ì™„ë£Œ: {output_refined_txt} (ì´ {len(final_sentences)} ë¬¸ì¥)")
    return final_sentences