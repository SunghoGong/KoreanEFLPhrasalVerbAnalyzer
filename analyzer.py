import pandas as pd
import stanza
import torch
import gc
from tqdm import tqdm
from collections import defaultdict

def run_phrasal_analysis(sentences, phrasal_verb_path, output_excel="result.xlsx"):
    """ì •ì œëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ì™€ êµ¬ë™ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ë¶„ì„ ìˆ˜í–‰"""
    print("ğŸ” êµ¬ë™ì‚¬ ë¶„ì„ ì‹œì‘...")

    # êµ¬ë™ì‚¬ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
    try:
        df_pv = pd.read_excel(phrasal_verb_path)
        pv_list = df_pv['Phrasal verb'].tolist()
        # setìœ¼ë¡œ ë§Œë“¤ì–´ ê²€ìƒ‰ ì†ë„ í–¥ìƒ
        # (ë¡œì§ìƒ lemma_key in verbDict ê²€ì‚¬ë¥¼ ìœ„í•´ dict ì´ˆê¸°í™”ê°€ í•„ìš”)
        verbDict = {each: 0 for each in pv_list}
        sentDict = {each: [] for each in pv_list}
    except Exception as e:
        print(f"âŒ êµ¬ë™ì‚¬ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # Stanza íŒŒì´í”„ë¼ì¸ ì„¤ì •
    stanza.download('en')
    nlp = stanza.Pipeline('en', processors='tokenize,lemma,pos,depparse', 
                          tokenize_no_ssplit=True, verbose=True, use_gpu=True)
    
    TARGET_DEPS = {'prt', 'advmod', 'compound:prt', 'prep'}
    batch_size = 1000

    # ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ ì •ë ¬ (ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨í™”)
    sentences.sort(key=len)

    # ì²­í¬ ë‚˜ëˆ„ê¸° í•¨ìˆ˜
    def list_chunk(lst, n):
        return [lst[i:i+n] for i in range(0, len(lst), n)]

    # 20ë§Œê°œ ë‹¨ìœ„ë¡œ ì²­í¬ ë¶„í•  (ë©”ëª¨ë¦¬ ê´€ë¦¬)
    chunks = list_chunk(sentences, 200000)

    print(f"  - ì´ {len(sentences)} ë¬¸ì¥, {len(chunks)}ê°œ ì²­í¬ë¡œ ì²˜ë¦¬")

    for idx, chunk in enumerate(chunks):
        print(f"  Processing Chunk {idx+1}/{len(chunks)}")
        
        for i in tqdm(range(0, len(chunk), batch_size)):
            batch = chunk[i : i + batch_size]
            docs = nlp(batch)
            
            for sentence in docs.sentences:
                for head, relation, dep in sentence.dependencies:
                    if head.upos == 'VERB' and relation in TARGET_DEPS:
                        lemma_key = f"{head.lemma} {dep.lemma}"
                        
                        if lemma_key in verbDict:
                            verbDict[lemma_key] += 1
                            sentDict[lemma_key].append(sentence.text)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del docs
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    # ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥
    print("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    unique_sentDict = {k: list(set(v)) for k, v in sentDict.items()} # ì¤‘ë³µ ë¬¸ì¥ ì œê±°
    
    df_count = pd.DataFrame(list(verbDict.items()), columns=['Verb', 'Count'])
    df_sentences = pd.DataFrame(list(unique_sentDict.items()), columns=['Verb', 'Sentences_List'])
    
    df_merged = pd.merge(df_count, df_sentences, on='Verb', how='inner')
    
    # ë¦¬ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ë¬¸ìì—´ë¡œ ë³€í™˜
    df_final = df_merged.assign(
        Sentences=lambda x: x['Sentences_List'].apply(lambda s: '\n'.join(s))
    ).drop(columns=['Sentences_List'])

    df_final.to_excel(output_excel, index=False)
    print(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {output_excel}")